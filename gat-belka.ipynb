{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67356,"databundleVersionId":8006601,"sourceType":"competition"},{"sourceId":8519891,"sourceType":"datasetVersion","datasetId":4784530}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Launch Kaggle TPU session\nprint('Session Start!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch_geometric\n!pip install indexed_bzip2\n!pip install rdkit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Run when Accelerator is GPU P100\nimport torch\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print('GPU available: ', torch.cuda.get_device_name(device))\nelse:\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print('GPU not available, using CPU')\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\nfrom torch.nn import Sequential, Linear, ReLU\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.nn import global_max_pool as gmp\nimport indexed_bzip2 as ibz2\nimport os\nimport pickle\nimport rdkit\nfrom rdkit import Chem\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n#from torch_scatter import scatter\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nimport gc","metadata":{}},{"cell_type":"code","source":"#Run when Accelerator is TPU VM v3-8\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.debug.metrics as met\nimport torch_xla.utils.utils as xu\nimport torch_xla.distributed.parallel_loader as pl\n\n# Check TPU availability\nassert xm.xrt_world_size() == 1, 'TPU not available or not properly configured'\nprint('TPU is available and properly configured')\n\n# Use TPU device\ndevice = xm.xla_device()\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\nfrom torch.nn import Sequential, Linear, ReLU\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.nn import global_max_pool as gmp\nimport indexed_bzip2 as ibz2\nimport os\nimport pickle\nimport rdkit\nfrom rdkit import Chem\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n#from torch_scatter import scatter\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nimport gc\nprint('import DONE!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load first 30m Train_graph\ndef load_compressed_ibz2_pickle(file):\n    with ibz2.open(file, parallelization=os.cpu_count()) as f:\n        data = pickle.load(f)\n    return data\ngdf_train = load_compressed_ibz2_pickle(\n    '/kaggle/input/leash-bio-processed-dataset/train-replace-c-30m.graph.pickle.b2z'\n)\nprint('train_graph Loaded!')\n\n# Load Train_bind\ntrainbind_data = np.load('/kaggle/input/leash-bio-processed-dataset/train.bind.npz')\ntrain_bind = trainbind_data['bind']\ntrainbind_data.close()\nprint('train_bind Loaded!')\n\nprint(len(gdf_train),len(train_bind))\nprint('DONE!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Run when loading full training dataset\n#gdf_train2 = load_compressed_ibz2_pickle(\n#    '/kaggle/input/leash-bio-processed-dataset/train-replace-c-30m.graph.pickle.01.b2z'\n#)\n#print('train_graph Loaded!')\n#print(len(gdf_train2))\n\n#gdf_train3 = load_compressed_ibz2_pickle(\n#    '/kaggle/input/leash-bio-processed-dataset/train-replace-c-30m.graph.pickle.02.b2z'\n#)\n#print('train_graph Loaded!')\n#print(len(gdf_train3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#merge train_graph datasets\ngdf_train_concat =  gdf_train #+ gdf_train2 + gdf_train3  #add train2 and train3 when loading full training dataset\nprint(\"DONE\",len(gdf_train_concat))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"zipped = list(zip(gdf_train_concat[:10000000],train_bind[:10000000]))\n\npos_class = [x for x in zipped if sum(x[1]) > 0]\nneg_class = [x for x in zipped if sum(x[1]) == 0]\nprint(len(pos_class),len(neg_class))\nprint(\"DONE!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#skip when training for submission\nimport random\nrandom.shuffle(pos_class)\nrandom.shuffle(neg_class)\n\ntrain_sample = pos_class[:100000] + neg_class[:100000]\ntrain_graph_sample = []\ntrain_bind_sample = []\n\nrandom.shuffle(train_sample)\n\nfor i in train_sample:\n    train_graph_sample.append(i[0])\n    train_bind_sample.append(i[1])\n    \n    \nprint('length of train split: ',len(train_graph_sample), len(train_bind_sample))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check train_bind \ntrain_bind_sample[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Helper: convert graph to pyg list\ndef to_pyg_list(graph):\n    L = len(graph)\n    for i in tqdm(range(L)):\n        N, edge, node_feature, edge_feature = graph[i]\n        graph[i] = Data(\n            idx=i,\n            edge_index=torch.from_numpy(edge.T).int(),\n            x=torch.from_numpy(node_feature).byte(),\n            edge_attr=torch.from_numpy(edge_feature).byte(),\n            y=torch.tensor(train_bind_sample[i])\n        )\n    return graph\n\n#Helper for test_split\ndef to_pyg_list_test(graph):\n    L = len(graph)\n    for i in tqdm(range(L)):\n        N, edge, node_feature, edge_feature = graph[i]\n        graph[i] = Data(\n            idx=i,\n            edge_index=torch.from_numpy(edge.T).int(),\n            x=torch.from_numpy(node_feature).byte(),\n            edge_attr=torch.from_numpy(edge_feature).byte()\n        )\n    return graph\n\n# Converted test_graph, with label\ndef to_pyg_list_tests_wlabel(graph):\n    L = len(graph)\n    for i in tqdm(range(L)):\n        N, edge, node_feature, edge_feature = graph[i]\n        graph[i] = Data(\n            idx=i,\n            edge_index=torch.from_numpy(edge.T).int(),\n            x=torch.from_numpy(node_feature).byte(),\n            edge_attr=torch.from_numpy(edge_feature).byte(),\n            y=torch.tensor(test_bind_sample[i])\n        )\n    return graph\nprint('DONE!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converted train-graph, with label\ntrain_graph = to_pyg_list(train_graph_sample)\n#test_graph_wlabel = to_pyg_list_tests_wlabel(test_graph_sample)\nprint(len(train_graph))\ntrain_graph[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GATNet(torch.nn.Module):\n    def __init__(self, num_features=9, n_output=3, output_dim=32, dropout=0.5):\n        super(GATNet, self).__init__()\n\n        self.gcn1 = GATConv(num_features, num_features, heads=10, dropout=dropout)\n        self.gcn2 = GATConv(num_features * 10, output_dim, dropout=dropout)\n        self.fc_g1 = nn.Linear(output_dim, output_dim)\n\n        # add batch normalization\n        self.bn1 = nn.BatchNorm1d(output_dim)\n\n        self.fc1 = nn.Linear(output_dim, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.out = nn.Linear(32, n_output)\n\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x.float(), data.edge_index, data.batch\n\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = F.elu(self.gcn1(x, edge_index))\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.gcn2(x, edge_index)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = gmp(x, batch)          \n        x = self.fc_g1(x)\n        x = self.relu(x)\n\n        # Dense layers\n        xc = self.fc1(x)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        xc = self.fc2(xc)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        out = self.out(xc)\n        return out\n\nmodel = GATNet()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  \nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n\ndef custom_loss(output, target):\n    output = output.view(-1)\n    target = target.view(-1)\n    return F.binary_cross_entropy_with_logits(output, target.float())\n\ndef train_model(training_set, batch_size=100000, epochs=8):\n    model.train()\n    dataloader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n    for epoch in range(epochs):\n        total_loss = 0\n        for data in dataloader:\n            optimizer.zero_grad()\n            output = model(data)\n            loss = custom_loss(output, data.y)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n            optimizer.step()\n            total_loss += loss.item()\n            \n        # update learning rate\n        scheduler.step()  \n        print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"model = model.to(device)\ndef train_model(training_set, batch_size=50000, epochs=8):\n    model.train()\n    dataloader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n    for epoch in range(epochs):\n        total_loss = 0\n        for data in dataloader:\n            data = data.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = custom_loss(output, data.y)\n            loss.backward()\n            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n            xm.optimizer_step(optimizer)\n            total_loss += loss.item()\n            \n        # update learning rate\n        xm.mark_step()  \n        print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}')","metadata":{}},{"cell_type":"code","source":"# For Debug\ntraining_set = train_graph#[:10000]#[Data(idx=-1, edge_index=None, x=torch.rand(5, 20), edge_attr=None, y=torch.tensor([0, 0, 1])) for _ in range(10)]\n\n# Train with the training_set\ntrain_model(training_set)\nprint('training DONE!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction function\ndef predict_label(data):\n    model.eval()\n    output = model(data)\n    labels = torch.sigmoid(output).detach().numpy()\n    #predicted_label = 0 if np.mean(labels)<0.1 else 1\n    return labels#predicted_label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Threshold\ntest_sample = pos_class[100000:110000] + neg_class[100000:110000]\ntest_graph_sample = []\ntest_bind_sample = []\n\nrandom.shuffle(train_sample)\n\nfor i in test_sample:\n    test_graph_sample.append(i[0])\n    test_bind_sample.append(i[1])\n    \nprint('length of test split: ',len(test_graph_sample), len(test_bind_sample))\n\n#Convert testset to graph object\ndef to_pyg_list_test(graph):\n    L = len(graph)\n    for i in tqdm(range(L)):\n        N, edge, node_feature, edge_feature = graph[i]\n        graph[i] = Data(\n            idx=i,\n            edge_index=torch.from_numpy(edge.T).int(),\n            x=torch.from_numpy(node_feature).byte(),\n            edge_attr=torch.from_numpy(edge_feature).byte(),\n            y=torch.tensor(test_bind_sample[i])\n        )\n    return graph\n\n#test_sample = pos_class[-5:]+neg_class[-5:]\ntest_graph_converted = to_pyg_list_test(test_graph_sample)\n\nprint('DONE!')\ntest_graph_converted[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import GATConv, global_mean_pool as gmp\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\n\ndef evaluate_model(test_set):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    \n    with torch.no_grad():\n        for data in test_set:\n            output = model(data)\n            output_sigmoid = output.sigmoid().numpy()\n            binary_labels = (np.sum(data.y.numpy()) > 0).astype(int)\n            all_labels.extend([binary_labels])\n            all_preds.extend(output_sigmoid.max(axis=1))  # Using max of three class, assuming positive correlation\n    \n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n    \n    #fpr, tpr, _ = roc_curve(all_labels, all_preds)\n    #roc_auc = auc(fpr, tpr)\n    \n    #plt.figure()\n    #plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n    #plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    #plt.xlim([0.0, 1.0])\n    #plt.ylim([0.0, 1.05])\n    #plt.xlabel('False Positive Rate')\n    #plt.ylabel('True Positive Rate')\n    #plt.title('Receiver Operating Characteristic')\n    #plt.legend(loc=\"lower right\")\n    #plt.show()\n    return all_labels, all_preds\n\nlanels_, preds_ = evaluate_model(test_graph_converted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check output in preds_\npreds_[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = np.round(preds_[0],4) - 0.03\nstepsize = 0.0001\nmatch_lst = []\nthreshold_lst = []\nhigh_thr, high_match = -1, -1\nlow_thr, low_match = 1, 1\ncounter, last_match = 0, 0\nwhile threshold <=np.round(preds_[0],4) + 0.03:\n    binary_pred_ = [1 if x <= threshold else 0 for x in preds_]\n    match = np.round(np.mean([1 if x==y else 0 for x,y in zip(binary_pred_, lanels_)]),4)\n\n        \n    if match > high_match: #update best-accuracy threshold\n        high_match = match\n        high_thr = np.round(threshold,5)\n        \n    elif match < low_match: #early stop if accuracy is dropping continuously\n        low_match = match\n        low_thr = np.round(threshold,5)\n        \n    match_lst.append(match)\n    threshold_lst.append(np.round(threshold,4))\n    threshold += stepsize\nprint('DONE')\n\nplt.figure()\nplt.plot(threshold_lst, match_lst, color='darkorange', lw=2, label='acc')\nplt.xlabel('Threshold')\nplt.ylabel('Accuracy')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(high_match, high_thr, low_match, low_thr)\nmatch_diff_h, match_diff_l = abs(high_match)-0.5, abs(low_match-0.5)\nprint(match_diff_h, match_diff_l)\nif high_match == -1 or match_diff_h < match_diff_l:\n    rev = 1\n\nelse:\n    rev = 0\n    \nrev\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction test\nimport random\nfor i in range(10):\n    print(predict_label(test_graph_converted[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert SMILES to graph objects\nPACK_NODE_DIM=9\nPACK_EDGE_DIM=1\nNODE_DIM=PACK_NODE_DIM*8\nEDGE_DIM=PACK_EDGE_DIM*8\n\ndef one_of_k_encoding(x, allowable_set, allow_unk=False):\n    if x not in allowable_set:\n        if allow_unk:\n            x = allowable_set[-1]\n        else:\n            raise Exception(f'input {x} not in allowable set{allowable_set}!!!')\n    return list(map(lambda s: x == s, allowable_set))\n\n\n#Get features of an atom (one-hot encoding:)\n'''\n    1.atom element: 44+1 dimensions    \n    2.the atom's hybridization: 5 dimensions\n    3.degree of atom: 6 dimensions                        \n    4.total number of H bound to atom: 6 dimensions\n    5.number of implicit H bound to atom: 6 dimensions    \n    6.whether the atom is on ring: 1 dimension\n    7.whether the atom is aromatic: 1 dimension           \n    Total: 70 dimensions\n'''\n\nATOM_SYMBOL = [\n    'C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg',\n    'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl',\n    'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H',\n    'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n    'Pt', 'Hg', 'Pb', 'Dy',\n    #'Unknown'\n]\n#print('ATOM_SYMBOL', len(ATOM_SYMBOL))44\nHYBRIDIZATION_TYPE = [\n    Chem.rdchem.HybridizationType.S,\n    Chem.rdchem.HybridizationType.SP,\n    Chem.rdchem.HybridizationType.SP2,\n    Chem.rdchem.HybridizationType.SP3,\n    Chem.rdchem.HybridizationType.SP3D\n]\n\ndef get_atom_feature(atom):\n    feature = (\n        one_of_k_encoding(atom.GetSymbol(), ATOM_SYMBOL)\n       + one_of_k_encoding(atom.GetHybridization(), HYBRIDIZATION_TYPE)\n       + one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5])\n       + one_of_k_encoding(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5])\n       + one_of_k_encoding(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5])\n       + [atom.IsInRing()]\n       + [atom.GetIsAromatic()]\n    )\n    #feature = np.array(feature, dtype=np.uint8)\n    feature = np.packbits(feature)\n    return feature\n\n\n#Get features of an edge (one-hot encoding)\n'''\n    1.single/double/triple/aromatic: 4 dimensions       \n    2.the atom's hybridization: 1 dimensions\n    3.whether the bond is on ring: 1 dimension          \n    Total: 6 dimensions\n'''\n\ndef get_bond_feature(bond):\n    bond_type = bond.GetBondType()\n    feature = [\n        bond_type == Chem.rdchem.BondType.SINGLE,\n        bond_type == Chem.rdchem.BondType.DOUBLE,\n        bond_type == Chem.rdchem.BondType.TRIPLE,\n        bond_type == Chem.rdchem.BondType.AROMATIC,\n        bond.GetIsConjugated(),\n        bond.IsInRing()\n    ]\n    #feature = np.array(feature, dtype=np.uint8)\n    feature = np.packbits(feature)\n    return feature\n\n\ndef smile_to_graph(smiles):\n    mol = Chem.MolFromSmiles(smiles)\n    N = mol.GetNumAtoms()\n    node_feature = []\n    edge_feature = []\n    edge = []\n    for i in range(mol.GetNumAtoms()):\n        atom_i = mol.GetAtomWithIdx(i)\n        atom_i_features = get_atom_feature(atom_i)\n        node_feature.append(atom_i_features)\n\n        for j in range(mol.GetNumAtoms()):\n            bond_ij = mol.GetBondBetweenAtoms(i, j)\n            if bond_ij is not None:\n                edge.append([i, j])\n                bond_features_ij = get_bond_feature(bond_ij)\n                edge_feature.append(bond_features_ij)\n    node_feature=np.stack(node_feature)\n    edge_feature=np.stack(edge_feature)\n    edge = np.array(edge,dtype=np.uint8)\n    return N,edge,node_feature,edge_feature\n\ndef to_pyg_format(N,edge,node_feature,edge_feature):\n    graph = Data(\n        idx=-1,\n        edge_index = torch.from_numpy(edge.T).int(),\n        x          = torch.from_numpy(node_feature).byte(),\n        edge_attr  = torch.from_numpy(edge_feature).byte(),\n    )\n    return graph\n\n#debug one example\ng = to_pyg_format(*smile_to_graph(smiles=\"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\"))\nprint(g)\nprint('[Dy] is replaced by C !!')\nprint('smile_to_graph() ok!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction function\ndef predict_label(data):\n    model.eval()\n    output = model(data)\n    labels = torch.sigmoid(output).detach().numpy()\n    #predicted_label = 0 if np.mean(labels)<0.1 else 1\n    return labels#predicted_label\n\ntest_file = '/kaggle/input/leash-BELKA/test.csv'\noutput_file = 'submission_GAT.csv'  # Specify the path and filename for the output file\ncount = 0\n# Read the test.parquet file into a pandas DataFrame\nfor df_test in pd.read_csv(test_file, chunksize=10000):\n    def helper_(s):\n        return to_pyg_format(*smile_to_graph(smiles=s))\n    #df_test.drop([\"buildingblock1_smiles\",\"buildingblock2_smiles\",\"buildingblock3_smiles\"],axis = 1)\n    df_test_graph = df_test['molecule_smiles'].apply(helper_)\n    \n\n    # \n    pred = df_test_graph.apply(predict_label)\n    \n    # Predict the probabilities\n    def helper2(pred_ar):\n        if rev == 0:\n            if pred_ar.max(axis=1)[0] <= high_thr:\n                return 1\n            return 0\n        else:\n            if pred_ar.max(axis=1)[0] <= low_thr:\n                return 0\n            return 1\n        \n        \n    probabilities = pred.apply(helper2)\n\n    # Create a DataFrame with 'id' and 'probability' columns\n    output_df = pd.DataFrame({'id': df_test['id'], 'binds': probabilities})\n\n    # Save the output DataFrame to a CSV file\n    output_df.to_csv(output_file, index=False, mode='a', header=not os.path.exists(output_file))\n    count += 1\n    print('iteration completed: ',count)\nprint('Prediction DONE! Result saved to ',output_file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}