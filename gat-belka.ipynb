{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":67356,"databundleVersionId":8006601,"sourceType":"competition"},{"sourceId":8519891,"sourceType":"datasetVersion","datasetId":4784530}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Launch Kaggle TPU session\nprint('Session Start!')","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:37:27.791520Z","iopub.execute_input":"2024-05-31T01:37:27.791783Z","iopub.status.idle":"2024-05-31T01:37:27.801311Z","shell.execute_reply.started":"2024-05-31T01:37:27.791754Z","shell.execute_reply":"2024-05-31T01:37:27.800672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch_geometric\n!pip install indexed_bzip2\n!pip install rdkit","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:39:59.167916Z","iopub.execute_input":"2024-05-31T01:39:59.168462Z","iopub.status.idle":"2024-05-31T01:40:16.668008Z","shell.execute_reply.started":"2024-05-31T01:39:59.168428Z","shell.execute_reply":"2024-05-31T01:40:16.667205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.nn import MessagePassing, global_mean_pool\nfrom torch_geometric.nn import GATConv\nimport indexed_bzip2 as ibz2\nimport os\nimport pickle\nimport rdkit\nfrom rdkit import Chem\n#from torch_scatter import scatter\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nimport gc\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\nprint('import DONE!')","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:42:11.437979Z","iopub.execute_input":"2024-05-31T01:42:11.438517Z","iopub.status.idle":"2024-05-31T01:42:32.053519Z","shell.execute_reply.started":"2024-05-31T01:42:11.438455Z","shell.execute_reply":"2024-05-31T01:42:32.052747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Sequential, Linear, ReLU\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.nn import global_max_pool as gmp\n\nclass GATNet(torch.nn.Module):\n    def __init__(self, num_features=9, n_output=3,n_filters=32, embed_dim=128, output_dim=1, dropout=0.2):\n        super(GATNet, self).__init__()\n\n        # GATConv\n        self.gcn1 = GATConv(num_features, num_features * 10, heads=10, dropout=dropout)\n        self.gcn2 = GATConv(num_features * 100, output_dim, dropout=dropout)\n        self.fc_g1 = nn.Linear(output_dim, output_dim)\n\n\n        self.fc1 = nn.Linear(output_dim, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.out = nn.Linear(32, n_output)\n\n        # relu and dropout\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x.float(), data.edge_index, data.batch\n\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = F.elu(self.gcn1(x, edge_index))\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.gcn2(x, edge_index)\n        x = self.relu(x)\n        x = gmp(x, batch)          \n        x = self.fc_g1(x)\n        x = self.relu(x)\n\n        # dense layers\n        xc = self.fc1(x)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        xc = self.fc2(xc)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        out = self.out(xc)\n        return out\n    \nmodel = GATNet()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\ndef custom_loss(output, target):\n    loss = 0\n    for i in range(3):  \n        loss += F.binary_cross_entropy_with_logits(output[0][i], target[i].float())\n    return loss / 3  \n\n\n# Training\ndef train_model(training_set):\n    model.train()\n    for data in training_set:\n        optimizer.zero_grad()\n        output = model(data)\n        #print(output)\n        #print(data.y)\n        #print(output[0][2].float(), data.y[0].float())\n        loss = custom_loss(output, data.y)#F.binary_cross_entropy_with_logits(output, data.y.view(-1, 3))\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T03:23:23.145814Z","iopub.execute_input":"2024-05-31T03:23:23.146261Z","iopub.status.idle":"2024-05-31T03:23:23.163419Z","shell.execute_reply.started":"2024-05-31T03:23:23.146223Z","shell.execute_reply":"2024-05-31T03:23:23.162686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load Train_graph\ndef load_compressed_ibz2_pickle(file):\n    with ibz2.open(file, parallelization=os.cpu_count()) as f:\n        data = pickle.load(f)\n    return data\ngdf_train = load_compressed_ibz2_pickle(\n    '/kaggle/input/leash-bio-processed-dataset/train-replace-c-30m.graph.pickle.b2z'\n)\nprint('train_graph Loaded!')\nprint(len(gdf_train))","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:43:41.915731Z","iopub.execute_input":"2024-05-31T01:43:41.916200Z","iopub.status.idle":"2024-05-31T01:48:39.404299Z","shell.execute_reply.started":"2024-05-31T01:43:41.916167Z","shell.execute_reply":"2024-05-31T01:48:39.397533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Train_bind\ntrainbind_data = np.load('/kaggle/input/leash-bio-processed-dataset/train.bind.npz')\ntrain_bind = trainbind_data['bind']\ntrainbind_data.close()\nprint('train_bind Loaded!')\nprint(len(train_bind))","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:58:47.832622Z","iopub.execute_input":"2024-05-31T01:58:47.833003Z","iopub.status.idle":"2024-05-31T01:58:48.421817Z","shell.execute_reply.started":"2024-05-31T01:58:47.832969Z","shell.execute_reply":"2024-05-31T01:58:48.420844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create testing subset\ntrain_graph_sample = gdf_train[:60000]\ntrain_bind_sample = train_bind[:60000]\nprint(len(train_graph_sample),len(train_bind_sample))","metadata":{"execution":{"iopub.status.busy":"2024-05-31T02:03:37.308616Z","iopub.execute_input":"2024-05-31T02:03:37.308931Z","iopub.status.idle":"2024-05-31T02:03:37.314680Z","shell.execute_reply.started":"2024-05-31T02:03:37.308903Z","shell.execute_reply":"2024-05-31T02:03:37.314076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_bind_sample[:5]","metadata":{"execution":{"iopub.status.busy":"2024-05-31T01:59:13.566159Z","iopub.execute_input":"2024-05-31T01:59:13.566541Z","iopub.status.idle":"2024-05-31T01:59:13.575429Z","shell.execute_reply.started":"2024-05-31T01:59:13.566511Z","shell.execute_reply":"2024-05-31T01:59:13.574413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Helper: convert graph to pyg list\ndef to_pyg_list(graph):\n    L = len(graph)\n    for i in tqdm(range(L)):\n        N, edge, node_feature, edge_feature = graph[i]\n        graph[i] = Data(\n            idx=i,\n            edge_index=torch.from_numpy(edge.T).int(),\n            x=torch.from_numpy(node_feature).byte(),\n            edge_attr=torch.from_numpy(edge_feature).byte(),\n            y=torch.tensor(train_bind_sample[i])\n        )\n    return graph\n\n#Helper for test_split\ndef to_pyg_list_test(graph):\n    L = len(graph)\n    for i in tqdm(range(L)):\n        N, edge, node_feature, edge_feature = graph[i]\n        graph[i] = Data(\n            idx=i,\n            edge_index=torch.from_numpy(edge.T).int(),\n            x=torch.from_numpy(node_feature).byte(),\n            edge_attr=torch.from_numpy(edge_feature).byte()\n        )\n    return graph","metadata":{"execution":{"iopub.status.busy":"2024-05-31T03:28:07.820312Z","iopub.execute_input":"2024-05-31T03:28:07.820751Z","iopub.status.idle":"2024-05-31T03:28:07.828070Z","shell.execute_reply.started":"2024-05-31T03:28:07.820715Z","shell.execute_reply":"2024-05-31T03:28:07.827316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converted train-graph, with label\ntrain_graph = to_pyg_list(train_graph_sample)\ntrain_graph[:5]","metadata":{"execution":{"iopub.status.busy":"2024-05-31T02:05:20.130908Z","iopub.execute_input":"2024-05-31T02:05:20.131283Z","iopub.status.idle":"2024-05-31T02:05:27.298493Z","shell.execute_reply.started":"2024-05-31T02:05:20.131253Z","shell.execute_reply":"2024-05-31T02:05:27.297806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create sample training_set of torch_geometric.data objects\ntraining_set = train_graph[:1000]#[Data(idx=-1, edge_index=None, x=torch.rand(5, 20), edge_attr=None, y=torch.tensor([0, 0, 1])) for _ in range(10)]\n\n# Train with the training_set\ntrain_model(training_set)\nprint('training DONE!')\n\n# Prediction\ndef predict_label(data):\n    model.eval()\n    output = model(data)\n    labels = torch.sigmoid(output).detach().numpy()\n    predicted_label = 0 if np.mean(labels)<0.1 else 1\n    return predicted_label","metadata":{"execution":{"iopub.status.busy":"2024-05-31T03:36:13.762208Z","iopub.execute_input":"2024-05-31T03:36:13.762886Z","iopub.status.idle":"2024-05-31T03:36:31.773149Z","shell.execute_reply.started":"2024-05-31T03:36:13.762847Z","shell.execute_reply":"2024-05-31T03:36:31.771949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction test\npredict_label(to_pyg_list_test([test_item])[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-31T03:36:56.697664Z","iopub.execute_input":"2024-05-31T03:36:56.698092Z","iopub.status.idle":"2024-05-31T03:36:56.712224Z","shell.execute_reply.started":"2024-05-31T03:36:56.698057Z","shell.execute_reply":"2024-05-31T03:36:56.711224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Playground","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model with the training_set\ntrain_model(training_set)\n\n# Create a new torch_geometric.data object for prediction\ndata_to_predict = Data(idx=-1, edge_index=None, x=torch.rand(5, 20), edge_attr=None)\n\n# Make predictions using the model\npredicted_label = predict_label(data_to_predict)\nprint(predicted_label)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GAT(nn.Module):\n    def __init__(self, in_features, out_features, num_heads):\n        super(GAT, self).__init__()\n        self.conv1 = GATConv(in_features, out_features, heads=num_heads)\n        self.fc = nn.Linear(out_features*num_heads, 3)\n        \n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = F.elu(self.conv1(x, edge_index))\n        x = F.dropout(x, training=self.training)\n        x = self.fc(x)\n        return x\n\n# Define the model\nmodel = GAT(in_features=20, out_features=8, num_heads=3)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Train the model\ndef train_model(training_set):\n    model.train()\n    for data in training_set:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.binary_cross_entropy_with_logits(output, data.y.view(-1, 3))\n        loss.backward()\n        optimizer.step()\n\n# Predict the label\ndef predict_label(data):\n    model.eval()\n    output = model(data)\n    predicted_label = torch.sigmoid(output).detach().numpy()\n    return predicted_label\n\n# Create a sample training_set of torch_geometric.data objects\ntraining_set = [Data(idx=-1, edge_index=None, x=torch.rand(5, 20), edge_attr=None, y=torch.tensor([0, 0, 1])) for _ in range(10)]\n\n# Train the model with the training_set\ntrain_model(training_set)\n\n# Create a new torch_geometric.data object for prediction\ndata_to_predict = Data(idx=-1, edge_index=None, x=torch.rand(5, 20), edge_attr=None)\n\n# Make predictions using the model\npredicted_label = predict_label(data_to_predict)\nprint(predicted_label)","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:07:34.740979Z","iopub.execute_input":"2024-05-30T11:07:34.741335Z","iopub.status.idle":"2024-05-30T11:07:34.889190Z","shell.execute_reply.started":"2024-05-30T11:07:34.741306Z","shell.execute_reply":"2024-05-30T11:07:34.888020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nimport rdkit\nfrom rdkit import Chem\n\nimport torch\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\n\nfrom torch_geometric.nn import MessagePassing, global_mean_pool\nfrom torch_scatter import scatter\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nprint('import ok!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper\n# torch version of np unpackbits\n#https://gist.github.com/vadimkantorov/30ea6d278bc492abf6ad328c6965613a\n\ndef tensor_dim_slice(tensor, dim, dim_slice):\n\treturn tensor[(dim if dim >= 0 else dim + tensor.dim()) * (slice(None),) + (dim_slice,)]\n\n# @torch.jit.script\ndef packshape(shape, dim: int = -1, mask: int = 0b00000001, dtype=torch.uint8, pack=True):\n\tdim = dim if dim >= 0 else dim + len(shape)\n\tbits, nibble = (\n\t\t8 if dtype is torch.uint8 else 16 if dtype is torch.int16 else 32 if dtype is torch.int32 else 64 if dtype is torch.int64 else 0), (\n\t\t1 if mask == 0b00000001 else 2 if mask == 0b00000011 else 4 if mask == 0b00001111 else 8 if mask == 0b11111111 else 0)\n\t# bits = torch.iinfo(dtype).bits # does not JIT compile\n\tassert nibble <= bits and bits % nibble == 0\n\tnibbles = bits // nibble\n\tshape = (shape[:dim] + (int(math.ceil(shape[dim] / nibbles)),) + shape[1 + dim:]) if pack else (\n\t\t\t\tshape[:dim] + (shape[dim] * nibbles,) + shape[1 + dim:])\n\treturn shape, nibbles, nibble\n\n# @torch.jit.script\ndef F_unpackbits(tensor, dim: int = -1, mask: int = 0b00000001, shape=None, out=None, dtype=torch.uint8):\n\tdim = dim if dim >= 0 else dim + tensor.dim()\n\tshape_, nibbles, nibble = packshape(tensor.shape, dim=dim, mask=mask, dtype=tensor.dtype, pack=False)\n\tshape = shape if shape is not None else shape_\n\tout = out if out is not None else torch.empty(shape, device=tensor.device, dtype=dtype)\n\tassert out.shape == shape\n\n\tif shape[dim] % nibbles == 0:\n\t\tshift = torch.arange((nibbles - 1) * nibble, -1, -nibble, dtype=torch.uint8, device=tensor.device)\n\t\tshift = shift.view(nibbles, *((1,) * (tensor.dim() - dim - 1)))\n\t\treturn torch.bitwise_and((tensor.unsqueeze(1 + dim) >> shift).view_as(out), mask, out=out)\n\n\telse:\n\t\tfor i in range(nibbles):\n\t\t\tshift = nibble * i\n\t\t\tsliced_output = tensor_dim_slice(out, dim, slice(i, None, nibbles))\n\t\t\tsliced_input = tensor.narrow(dim, 0, sliced_output.shape[dim])\n\t\t\ttorch.bitwise_and(sliced_input >> shift, mask, out=sliced_output)\n\treturn out\n\nclass dotdict(dict):\n\t__setattr__ = dict.__setitem__\n\t__delattr__ = dict.__delitem__\n\t\n\tdef __getattr__(self, name):\n\t\ttry:\n\t\t\treturn self[name]\n\t\texcept KeyError:\n\t\t\traise AttributeError(name)\n\n            \nprint('helper ok!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# mol to graph adopted from\n# from https://github.com/LiZhang30/GPCNDTA/blob/main/utils/DrugGraph.py\n\nPACK_NODE_DIM=9\nPACK_EDGE_DIM=1\nNODE_DIM=PACK_NODE_DIM*8\nEDGE_DIM=PACK_EDGE_DIM*8\n\ndef one_of_k_encoding(x, allowable_set, allow_unk=False):\n\tif x not in allowable_set:\n\t\tif allow_unk:\n\t\t\tx = allowable_set[-1]\n\t\telse:\n\t\t\traise Exception(f'input {x} not in allowable set{allowable_set}!!!')\n\treturn list(map(lambda s: x == s, allowable_set))\n\n\n#Get features of an atom (one-hot encoding:)\n'''\n\t1.atom element: 44+1 dimensions    \n\t2.the atom's hybridization: 5 dimensions\n\t3.degree of atom: 6 dimensions                        \n\t4.total number of H bound to atom: 6 dimensions\n\t5.number of implicit H bound to atom: 6 dimensions    \n\t6.whether the atom is on ring: 1 dimension\n\t7.whether the atom is aromatic: 1 dimension           \n\tTotal: 70 dimensions\n'''\n\nATOM_SYMBOL = [\n\t'C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg',\n\t'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl',\n\t'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H',\n\t'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n\t'Pt', 'Hg', 'Pb', 'Dy',\n\t#'Unknown'\n]\n#print('ATOM_SYMBOL', len(ATOM_SYMBOL))44\nHYBRIDIZATION_TYPE = [\n\tChem.rdchem.HybridizationType.S,\n\tChem.rdchem.HybridizationType.SP,\n\tChem.rdchem.HybridizationType.SP2,\n\tChem.rdchem.HybridizationType.SP3,\n\tChem.rdchem.HybridizationType.SP3D\n]\n\ndef get_atom_feature(atom):\n\tfeature = (\n\t\t one_of_k_encoding(atom.GetSymbol(), ATOM_SYMBOL)\n\t   + one_of_k_encoding(atom.GetHybridization(), HYBRIDIZATION_TYPE)\n\t   + one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5])\n\t   + one_of_k_encoding(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5])\n\t   + one_of_k_encoding(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5])\n\t   + [atom.IsInRing()]\n\t   + [atom.GetIsAromatic()]\n\t)\n\t#feature = np.array(feature, dtype=np.uint8)\n\tfeature = np.packbits(feature)\n\treturn feature\n\n\n#Get features of an edge (one-hot encoding)\n'''\n\t1.single/double/triple/aromatic: 4 dimensions       \n\t2.the atom's hybridization: 1 dimensions\n\t3.whether the bond is on ring: 1 dimension          \n\tTotal: 6 dimensions\n'''\n\ndef get_bond_feature(bond):\n\tbond_type = bond.GetBondType()\n\tfeature = [\n\t\tbond_type == Chem.rdchem.BondType.SINGLE,\n\t\tbond_type == Chem.rdchem.BondType.DOUBLE,\n\t\tbond_type == Chem.rdchem.BondType.TRIPLE,\n\t\tbond_type == Chem.rdchem.BondType.AROMATIC,\n\t\tbond.GetIsConjugated(),\n\t\tbond.IsInRing()\n\t]\n\t#feature = np.array(feature, dtype=np.uint8)\n\tfeature = np.packbits(feature)\n\treturn feature\n\n\ndef smile_to_graph(smiles):\n\tmol = Chem.MolFromSmiles(smiles)\n\tN = mol.GetNumAtoms()\n\tnode_feature = []\n\tedge_feature = []\n\tedge = []\n\tfor i in range(mol.GetNumAtoms()):\n\t\tatom_i = mol.GetAtomWithIdx(i)\n\t\tatom_i_features = get_atom_feature(atom_i)\n\t\tnode_feature.append(atom_i_features)\n\n\t\tfor j in range(mol.GetNumAtoms()):\n\t\t\tbond_ij = mol.GetBondBetweenAtoms(i, j)\n\t\t\tif bond_ij is not None:\n\t\t\t\tedge.append([i, j])\n\t\t\t\tbond_features_ij = get_bond_feature(bond_ij)\n\t\t\t\tedge_feature.append(bond_features_ij)\n\tnode_feature=np.stack(node_feature)\n\tedge_feature=np.stack(edge_feature)\n\tedge = np.array(edge,dtype=np.uint8)\n\treturn N,edge,node_feature,edge_feature\n\ndef to_pyg_format(N,edge,node_feature,edge_feature):\n\tgraph = Data(\n\t\tidx=-1,\n\t\tedge_index = torch.from_numpy(edge.T).int(),\n\t\tx          = torch.from_numpy(node_feature).byte(),\n\t\tedge_attr  = torch.from_numpy(edge_feature).byte(),\n\t)\n\treturn graph\n\n#debug one example\ng = to_pyg_format(*smile_to_graph(smiles=\"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\"))\nprint(g)\nprint('[Dy] is replaced by C !!')\nprint('smile_to_graph() ok!')","metadata":{"execution":{"iopub.status.busy":"2024-05-29T06:53:02.452061Z","iopub.execute_input":"2024-05-29T06:53:02.452583Z","iopub.status.idle":"2024-05-29T06:53:02.510427Z","shell.execute_reply.started":"2024-05-29T06:53:02.452550Z","shell.execute_reply":"2024-05-29T06:53:02.508128Z"}}},{"cell_type":"code","source":"print('DONE')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nimport rdkit\nfrom rdkit import Chem\n\nimport torch\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\n\nfrom torch_geometric.nn import MessagePassing, global_mean_pool\nfrom torch_scatter import scatter\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nprint('import ok!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install rdkit","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE='cpu'\n\n# i have removed all comments here to jepp it clean. refer to orginal link for code comments\n# of MPNNModel\nclass MPNNLayer(MessagePassing):\n\tdef __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n\t\tsuper().__init__(aggr=aggr)\n\n\t\tself.emb_dim = emb_dim\n\t\tself.edge_dim = edge_dim\n\t\tself.mlp_msg = nn.Sequential(\n\t\t\tnn.Linear(2 * emb_dim + edge_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(),\n\t\t\tnn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU()\n\t\t)\n\t\tself.mlp_upd = nn.Sequential(\n\t\t\tnn.Linear(2 * emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU(),\n\t\t\tnn.Linear(emb_dim, emb_dim), nn.BatchNorm1d(emb_dim), nn.ReLU()\n\t\t)\n\n\tdef forward(self, h, edge_index, edge_attr):\n\t\tout = self.propagate(edge_index, h=h, edge_attr=edge_attr)\n\t\treturn out\n\n\tdef message(self, h_i, h_j, edge_attr):\n\t\tmsg = torch.cat([h_i, h_j, edge_attr], dim=-1)\n\t\treturn self.mlp_msg(msg)\n\n\tdef aggregate(self, inputs, index):\n\t\treturn scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n\n\tdef update(self, aggr_out, h):\n\t\tupd_out = torch.cat([h, aggr_out], dim=-1)\n\t\treturn self.mlp_upd(upd_out)\n\n\tdef __repr__(self) -> str:\n\t\treturn (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')\n\n\nclass MPNNModel(nn.Module):\n    def __init__(self, num_layers=3, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n        super().__init__()\n\n        self.lin_in = nn.Linear(in_dim, emb_dim)\n\n        # Stack of MPNN layers\n        self.convs = torch.nn.ModuleList()\n        for layer in range(num_layers):\n            self.convs.append(MPNNLayer(emb_dim, edge_dim, aggr='add'))\n\n        self.pool = global_mean_pool\n\n    def forward(self, data): #PyG.Data - batch of PyG graphs\n\n        h = self.lin_in(F_unpackbits(data.x,-1).float())  \n\n        for conv in self.convs:\n            h = h + conv(h, data.edge_index.long(), F_unpackbits(data.edge_attr,-1).float())  # (n, d) -> (n, d)\n\n        h_graph = self.pool(h, data.batch)  \n        return h_graph\n\n# our prediction model here !!!!\nclass Net(nn.Module):\n    def __init__(self, ):\n        super().__init__()\n\n        self.output_type = ['infer', 'loss']\n\n        graph_dim=96\n        self.smile_encoder = MPNNModel(\n            in_dim=NODE_DIM, edge_dim=EDGE_DIM, emb_dim=graph_dim, num_layers=4,\n        )\n        self.bind = nn.Sequential(\n            nn.Linear(graph_dim, 1024),\n            #nn.BatchNorm1d(1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(1024, 1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(1024, 512),\n            #nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n            nn.Linear(512, 3),\n        )\n\n    def forward(self, batch):\n        graph = batch['graph']\n        x = self.smile_encoder(graph) \n        bind = self.bind(x)\n\n        # --------------------------\n        output = {}\n        if 'loss' in self.output_type:\n            target = batch['bind']\n            output['bce_loss'] = F.binary_cross_entropy_with_logits(bind.float(), target.float())\n        if 'infer' in self.output_type:\n            output['bind'] = torch.sigmoid(bind)\n\n        return output\n    \nprint('Create Model OK!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#debug: make some dummy data and run\n\ndef run_check_net():\n\tbatch_size = 3\n\tnode_dim=NODE_DIM\n\tedge_dim=EDGE_DIM\n\n\tdata = []\n\tfor b in range(batch_size):\n\t\tN = np.random.randint(5,10)\n\t\tE = np.random.randint(3,N*(N-1))\n\t\tedge_index = np.stack([\n\t\t\tnp.random.choice(N, E, replace=True),\n\t\t\tnp.random.choice(N, E, replace=True),\n\t\t]).T\n\t\tedge_index = np.sort(edge_index)\n\t\tedge_index = edge_index[edge_index[:, 0].argsort()]\n\t\tedge_index[0] = [0,1] #default\n\t\tedge_index = edge_index[edge_index[:,0]!=edge_index[:,1]]\n\t\tedge_index = np.unique(edge_index, axis=0)\n\n\t\tE = len(edge_index)\n\t\tedge_index = np.ascontiguousarray(edge_index.T)\n\n\t\td = Data(\n\t\t\tidx        = b,\n\t\t\tedge_index = torch.from_numpy(edge_index).int(),\n\t\t\tx          = torch.from_numpy(np.packbits(np.random.choice(2, (N, node_dim)),-1)).byte(),\n\t\t\tedge_attr  = torch.from_numpy(np.packbits(np.random.choice(2, (E, edge_dim)),-1)).byte(),\n\t\t)\n\t\tdata.append(d)\n\n\t#from my_mol2graph import make_dummy_data\n\t#data = make_dummy_data()\n\n\tloader = DataLoader(data, batch_size=batch_size)\n\tgraph = next(iter(loader))\n\tidx = graph.idx.tolist()  #use to index bind array\n\tbatch = dotdict( \n\t\tgraph = graph.to(DEVICE),\n\t\tbind  = torch.from_numpy(np.random.choice(2, (batch_size, 3))).float().to(DEVICE),\n\t)\n\tzz=0\n \n\tnet = Net().to(DEVICE)\n\t#print(net)\n\n\twith torch.no_grad():\n\t\twith torch.cuda.amp.autocast(enabled=True): # dtype=torch.float16):\n\t\t\toutput = net(batch)\n\t\t\t#print(output['bind'])\n\n\t# ---\n\tprint('batch')\n\tfor k, v in batch.items():\n\t\tif k=='idx':\n\t\t\tprint(f'{k:>32} : {len(v)} ')\n\t\telif k=='graph':\n\t\t\tprint(f'{k:>32} : {graph} ')\n\t\telse:\n\t\t\tprint(f'{k:>32} : {v.shape} ')\n\n\tprint('output')\n\tfor k, v in output.items():\n\t\tif 'loss' not in k:\n\t\t\tprint(f'{k:>32} : {v.shape} ')\n\tprint('loss')\n\tfor k, v in output.items():\n\t\tif 'loss' in k:\n\t\t\tprint(f'{k:>32} : {v.item()} ')\n\n            \nrun_check_net()\nprint('model ok!')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_collate(graph, index=None, device='cpu'):\n    if index is None:\n        index = np.arange(len(graph)).tolist()\n    batch = dotdict(\n        x=[],\n        edge_index=[],\n        edge_attr=[],\n        batch=[],\n        idx=index\n    )\n    offset = 0\n    for b, i in enumerate(index):\n        N, edge, node_feature, edge_feature = graph[i]\n        batch.x.append(node_feature)\n        batch.edge_attr.append(edge_feature)\n        batch.edge_index.append(edge.astype(int) + offset)\n        batch.batch += N * [b]\n        offset += N\n    batch.x = torch.from_numpy(np.concatenate(batch.x)).to(device)\n    batch.edge_attr = torch.from_numpy(np.concatenate(batch.edge_attr)).to(device)\n    batch.edge_index = torch.from_numpy(np.concatenate(batch.edge_index).T).to(device)\n    batch.batch = torch.LongTensor(batch.batch).to(device)\n    return batch\n\n\n#.... more code here ....\n\nwhile epoch<cfg.num_epoch:\n    shuffled_idx = train_idx.copy()\n    np.random.shuffle(shuffled_idx)\n    for t, index in enumerate(np.arange(0,len(shuffled_idx),cfg.train_batch_size)):\n        index = shuffled_idx[index:index+cfg.train_batch_size]\n        if len(index)!=cfg.train_batch_size: continue #drop last\n\n        B = len(index)\n        batch = dotdict(\n            graph = my_collate(train_graph,index,device='cuda'),\n            bind = torch.from_numpy(train_bind[index]).float().cuda(),\n        )\n\n        net.train()\n        net.output_type = ['loss', 'infer']\n\n        with torch.cuda.amp.autocast(enabled=cfg.is_amp):\n            output = net(batch)  #data_parallel(net,batch) #\n            bce_loss = output['bce_loss']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from multiprocessing import Pool\nfrom tqdm import tqdm\nimport gc\nfrom torch_geometric.loader import DataLoader as PyGDataLoader\n\ndef to_pyg_list(graph):\n\tL = len(graph)\n\tfor i in tqdm(range(L)):\n\t\tN, edge, node_feature, edge_feature = graph[i]\n\t\tgraph[i] = Data(\n\t\t\tidx=i,\n\t\t\tedge_index=torch.from_numpy(edge.T).int(),\n\t\t\tx=torch.from_numpy(node_feature).byte(),\n\t\t\tedge_attr=torch.from_numpy(edge_feature).byte(),\n\t\t)\n\treturn graph\n\n\ntrain_smiles=[ #replace [Dy] with C\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\n    \"C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3)nc(N[C@@H](CC#C)CC(=O)NC)n2)cc1\",\n]\ntrain_bind =np.array([\n    [0,0,0],[1,0,0],[0,1,0],[0,0,1],[1,1,0],[0,0,0],\n])\nnum_train= len(train_smiles)\nwith Pool(processes=64) as pool:\n    train_graph = list(tqdm(pool.imap(smile_to_graph, train_smiles), total=num_train))\n\ntrain_graph = to_pyg_list(train_graph)\ntrain_loader = PyGDataLoader(train_graph, batch_size=3, shuffle=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## example training loop\nscaler = torch.cuda.amp.GradScaler(enabled=True)\nnet = Net()\nnet.to(DEVICE)\n\noptimizer =\\\n\ttorch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001)\n\nnum_epoch=10\nepoch=0\niteration=0\nwhile epoch<num_epoch: \n\tfor t, graph_batch in enumerate(train_loader): \n\t\tindex = graph_batch.idx.tolist()\n\t\tB = len(index)\n\t\tbatch = dotdict(\n\t\t\tgraph  = graph_batch.to(DEVICE),\n\t\t\tbind   = torch.from_numpy(train_bind[index]).to(DEVICE),\n\t\t)\n\n\t\tnet.train()\n\t\tnet.output_type = ['loss', 'infer']\n\t\twith torch.cuda.amp.autocast(enabled=True):\n\t\t\toutput = net(batch)  #data_parallel(net,batch) #\n\t\t\tbce_loss = output['bce_loss']\n\n\t\toptimizer.zero_grad() \n\t\tscaler.scale(bce_loss).backward() \n\t\tscaler.step(optimizer)\n\t\tscaler.update()\n\t\t \n\t\ttorch.clear_autocast_cache()\n\t\tprint(epoch,iteration,bce_loss.item())\n\t\titeration +=  1\n        \n\tepoch += 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path ='/kaggle/input/leash-BELKA/train.parquet'\n\ncon = duckdb.connect()\ndf_train = con.query(f\"\"\"(SELECT *\n                        FROM parquet_scan('{train_path}')\n                        WHERE binds = 0\n                        ORDER BY random()\n                        LIMIT 30000)\n                        UNION ALL\n                        (SELECT *\n                        FROM parquet_scan('{train_path}')\n                        WHERE binds = 1\n                        ORDER BY random()\n                        LIMIT 30000)\"\"\").df()\ncon.close()\nprint('DONE!')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Sequential, Linear, ReLU\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.nn import global_max_pool as gmp\n\n# GAT  model\nclass GATNet(torch.nn.Module):\n    def __init__(self, num_features=112, n_output=1,n_filters=32, embed_dim=128, output_dim=128, dropout=0.2):\n        super(GATNet, self).__init__()\n\n        # graph layers\n        self.gcn1 = GATConv(num_features, num_features, heads=10, dropout=dropout)\n        self.gcn2 = GATConv(num_features * 10, output_dim, dropout=dropout)\n        self.fc_g1 = nn.Linear(output_dim, output_dim)\n\n        # combined layers\n        self.fc1 = nn.Linear(output_dim, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.out = nn.Linear(32, n_output)\n\n        # activation and regularization\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        # graph input feed-forward\n        x, edge_index, batch = data.x.float(), data.edge_index, data.batch\n\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = F.elu(self.gcn1(x, edge_index))\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.gcn2(x, edge_index)\n        x = self.relu(x)\n        x = gmp(x, batch)          # global max pooling\n        x = self.fc_g1(x)\n        x = self.relu(x)\n\n       \n        # add some dense layers\n        xc = self.fc1(x)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        xc = self.fc2(xc)\n        xc = self.relu(xc)\n        xc = self.dropout(xc)\n        out = self.out(xc)\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}